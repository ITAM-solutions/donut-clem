{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e43d94f-47fa-428f-93bc-2fb4ddd5615f",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "\n",
    "## NOTE: REQUIREMENTS\n",
    "* Linux kernel version: >=5.5. Check with `uname -r`.\n",
    "* tesseract. Install with `sudo apt install tesseract-ocr -y`.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3650b4-8c7e-4ace-9f92-628fedcbbc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch torchaudio torchvision -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ffd70-cc1d-4614-9013-5bc252bf1094",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchaudio torchvision transformers[torch] pytesseract tesseract matplotlib Pillow numpy tqdm scikit-learn protobuf sentencepiece pytorch-lightning tensorboardX accelerate nltk wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b5e5cd-d9a5-4e54-a5cb-f3169c43b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which tesseract  # Make sure that tesseract is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec9297-692e-46cc-b1e7-7574fb1a3f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e27788-eb55-48cd-8429-128ef294bff8",
   "metadata": {},
   "source": [
    "# Preparing Hugging Face connection\n",
    "\n",
    "This will allow to upload the training results to a private HF repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81129da-c8cc-4d23-9d94-1c1b801b86e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()  # Use itam-franmr account token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a558285-97ac-41b5-8aae-82d8433e7c77",
   "metadata": {},
   "source": [
    "# Define Donut Model and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d829d29-a33d-4529-b57f-b58ac86735e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel, VisionEncoderDecoderConfig\n",
    "import torch\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Configuration parameters\n",
    "model_name = \"naver-clova-ix/donut-base\"\n",
    "start_token = \"<s_cord-v2>\"\n",
    "end_token = \"</s_cord-v2>\"\n",
    "image_size = [1080, 720]\n",
    "max_length = 800\n",
    "\n",
    "# Defining processor\n",
    "processor = DonutProcessor.from_pretrained(model_name, use_fast=True)  # use_fast allows optimization\n",
    "processor.image_processor.do_align_long_axis = False\n",
    "processor.image_processor.size = {\"height\": image_size[0], \"width\": image_size[1]}\n",
    "\n",
    "# Defining Model\n",
    "model_config = VisionEncoderDecoderConfig.from_pretrained(model_name)\n",
    "model_config.encoder.image_size = image_size\n",
    "model_config.decoder.max_length = max_length\n",
    "model_config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model_config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(start_token)\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name, config=model_config)\n",
    "# model.to(device)  # Sends to GPU if existing.\n",
    "model.train()  # Activate training mode\n",
    "\n",
    "new_tokens_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfc7257-cd37-4633-979d-91ccf0edf025",
   "metadata": {},
   "source": [
    "# Define Donut Dataset\n",
    "\n",
    "Use PyTorch Dataset class to define a dataset in the format that Donut expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a3227-8cfb-4f74-b48d-65ca33da15a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from pathlib import Path, PureWindowsPath\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "class DonutDataset(Dataset):\n",
    "    def __init__(self,\n",
    "            dataset_path: Path,\n",
    "            processor,\n",
    "            model_config,\n",
    "            split: str = 'train',\n",
    "            start_token: str = \"<s>\",\n",
    "            end_token: str = \"</s>\",\n",
    "            excluded_keys: tuple = tuple()\n",
    "            ):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dataset_path = dataset_path\n",
    "        self.split = split\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.excluded_keys = excluded_keys\n",
    "        self.empty_token = '<s_no_value>'\n",
    "        self.ignore_id = -100\n",
    "        self.processor = processor\n",
    "        self.model_config = model_config\n",
    "        self.new_special_tokens = set()  # Set that will collect all new special tokens found in the given data.\n",
    "        \n",
    "        self.train_set = []\n",
    "        self.val_set = []\n",
    "        self.test_set = []\n",
    "\n",
    "        # Loads data from the given dataset path\n",
    "        self._get_synthetic_samples_list()\n",
    "        self.raw_samples = self._load_dataset()\n",
    "\n",
    "        # Add start and end tokens as special tokens.\n",
    "        self.new_special_tokens.add(self.start_token)\n",
    "        self.new_special_tokens.add(self.end_token)\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_train_validation_test(dataset: list):\n",
    "        \"\"\" Splits the data into training, validation and testing sets. \"\"\"\n",
    "        \n",
    "        x_train_tmp, x_test = train_test_split(dataset, test_size=0.1)\n",
    "        x_train, x_val= train_test_split(x_train_tmp, test_size=0.135)\n",
    "        return x_train, x_val, x_test\n",
    "\n",
    "    def _load_dataset(self) -> dict:\n",
    "        \"\"\" Selects a split from the dataset and loads its images and json contents as 'raw samples'. \"\"\"\n",
    "\n",
    "        samples_paths = self.train_set if self.split == 'train' \\\n",
    "            else self.val_set if self.split in ['validation', 'val'] \\\n",
    "            else self.test_set\n",
    "\n",
    "        raw_samples = {'ims': [], 'ids': [], 'json': []}\n",
    "\n",
    "        for raw_sample in tqdm(samples_paths):\n",
    "\n",
    "            im_path, json_path = raw_sample.with_suffix('.png'), raw_sample.with_suffix('.json')\n",
    "            \n",
    "            # Read image in RGB format.\n",
    "            im_raw = Image.open(im_path).convert('RGB')\n",
    "\n",
    "            # Read attached JSON content.\n",
    "            with open(json_path, 'r', encoding='utf-8') as fp:\n",
    "                json_content = json.load(fp)\n",
    "\n",
    "            raw_samples['json'].append(json_content)  # For debugging purposes\n",
    "            \n",
    "            json_content = self._clean_excluded_keys(json_content)\n",
    "            json_content = self._normalize_empty_values(json_content)\n",
    "            \n",
    "            json_tokens = self.json2token(json_content)\n",
    "\n",
    "            raw_samples['ims'].append(im_raw)\n",
    "            raw_samples['ids'].append(json_tokens)\n",
    "\n",
    "        return raw_samples\n",
    "\n",
    "    @staticmethod\n",
    "    def _clean_excluded_keys(obj: dict):\n",
    "        for k in self.excluded_keys:\n",
    "            if k in obj.keys():\n",
    "                obj.pop(k)\n",
    "\n",
    "        if obj.get('products'):\n",
    "            for k in self.excluded_keys:\n",
    "                if k in obj['products']:\n",
    "                    obj['products'].pop(k)\n",
    "\n",
    "        return obj\n",
    "\n",
    "    def _normalize_empty_values(self, obj: dict):\n",
    "        # Document-level fields\n",
    "        obj = {k: v if v else self.empty_token for k, v in obj.items()}\n",
    "\n",
    "        # Product-level fields\n",
    "        if type(obj['products']) == list:\n",
    "            for i, product in enumerate(obj['products']):\n",
    "                obj['products'][i] = {k: v if v else self.empty_token for k, v in product.items()}\n",
    "\n",
    "        return obj\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" \n",
    "        Retrieves the number of samples in the current dataset split. \n",
    "        **Method required by PyTorch DataLoaders to work**\n",
    "        \"\"\"\n",
    "\n",
    "        dataset_length = len(self.train_set) if self.split == 'train' \\\n",
    "                    else len(self.val_set) if self.split in ['validation', 'val'] \\\n",
    "                    else len(self.test_set)\n",
    "        return dataset_length                                                \n",
    "        \n",
    "    def __getitem__(self, idx) -> dict:\n",
    "        \"\"\" \n",
    "        Retrieves the sample in position `idx` from the dataset. \n",
    "        **Method required by PyTorch DataLoaders to work**\n",
    "        \"\"\"\n",
    "        \n",
    "        im_raw, json_tokens = self.raw_samples['ims'][idx], self.raw_samples['ids'][idx]\n",
    "\n",
    "        im = self._normalize_image(im_raw)\n",
    "        gt_tokens, target_str_sequence = self._normalize_tagging(json_tokens)\n",
    "\n",
    "        return {\n",
    "            'pixel_values': im,\n",
    "            'labels': gt_tokens,\n",
    "            'target_str_seq': target_str_sequence,\n",
    "        }        \n",
    "\n",
    "    def _get_samples_list(self):\n",
    "        \"\"\" \n",
    "        Finds all the samples in the dataset and splits them into training, validation and test sets\n",
    "        based on providers. The split percentages are applied to each provider, and not directly to \n",
    "        the full dataset, ensuring that if a provider has enough samples, they will exist in every data\n",
    "        split, making validation more reasonable.\n",
    "        \"\"\"\n",
    "\n",
    "        samples_by_provider = defaultdict(list)\n",
    "        metadata_files = self.dataset_path.glob('**/*.file_metadata.json')\n",
    "\n",
    "        # Iterate over sample folders to determine original providers.\n",
    "        for metadata_file in metadata_files:\n",
    "            with open(metadata_file, 'r') as fp:\n",
    "                json_content = json.load(fp)\n",
    "\n",
    "            original_file = PureWindowsPath(json_content['origin']['name'])  # Paths saved in a Windows machine.\n",
    "            provider = Path(*original_file.parts[:original_file.parts.index('datasets') + 3]).name\n",
    "\n",
    "            # Obtain the list of images for the current sample document.\n",
    "            samples_from_document = list(i.with_suffix('') for i in metadata_file.parent.glob('*.png'))\n",
    "            samples_by_provider[provider].extend(samples_from_document)\n",
    "\n",
    "        # Split providers into training, validation and test sets.\n",
    "        for provider, files in samples_by_provider.items():\n",
    "            if len(files) == 1:\n",
    "                self.train_set.extend(files)\n",
    "            elif len(files) == 2:\n",
    "                self.train_set.append(files[0])\n",
    "                self.val_set.append(files[1])\n",
    "            else:\n",
    "                train_set, val_set, test_set = self._split_train_validation_test(files)\n",
    "                self.train_set.extend(train_set)\n",
    "                self.val_set.extend(val_set)\n",
    "                self.test_set.extend(test_set)\n",
    "\n",
    "    def _get_synthetic_samples_list(self):\n",
    "        \"\"\"\n",
    "        Finds all the samples in a dataset with synthetic data and splits them into training,\n",
    "        validation and test sets based on the synthetic template used to create it. The split\n",
    "        percentages are applied to each template case, and not directly to the full dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        samples_by_template = defaultdict(list)\n",
    "        metadata_files = self.dataset_path.glob('**/*.file_metadata.json')\n",
    "\n",
    "        for metadata_file in metadata_files:\n",
    "            with open(metadata_file, 'r') as fp:\n",
    "                json_content = json.load(fp)\n",
    "\n",
    "            template_name = json_content['template_name']\n",
    "            samples_by_template[template_name].append(metadata_file.parent / metadata_file.parent.name)\n",
    "\n",
    "        for template_name, files in samples_by_template.items():\n",
    "            if len(files) == 1:\n",
    "                self.train_set.extend(files)\n",
    "            elif len(files) == 2:\n",
    "                self.train_set.append(files[0])\n",
    "                self.val_set.append(files[1])\n",
    "            else:\n",
    "                train_set, val_set, test_set = self._split_train_validation_test(files)\n",
    "                self.train_set.extend(train_set)\n",
    "                self.val_set.extend(val_set)\n",
    "                self.test_set.extend(test_set)\n",
    "        \n",
    "    def _normalize_image(self, im: Image) -> torch.Tensor:\n",
    "        \"\"\" Normalizes the given image to the expected tensor format for Donut. \"\"\"\n",
    "\n",
    "        im = self._fix_image_orientation(im)\n",
    "\n",
    "        # First dimension is removed, as it refers to the batch. Batches are automatically created by DataLoader.\n",
    "        im_tensor = processor(im, return_tensors='pt').pixel_values.squeeze()\n",
    "        return im_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def _fix_image_orientation(image: Image) -> Image:\n",
    "        \"\"\"\n",
    "        Uses pytesseract to get some image metadata, then get the page orientation, and finally rotate the image\n",
    "        when needed. If pytesseract cannot get the orientation, then the image is returned with no changes.\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            im_metadata = pytesseract.image_to_osd(image)\n",
    "            angle = 360 - int(re.search(r'(?<=Rotate: )\\d+', im_metadata).group(0))\n",
    "            return image.rotate(angle, expand=1)\n",
    "        except Exception:\n",
    "            # Tesseract failed. Return image in its original format.\n",
    "            return image\n",
    "\n",
    "    def _normalize_tagging(self, json_tokens) -> tuple[torch.Tensor, str]:\n",
    "        \"\"\" Converts the JSON content to tokens that can be fed to Donut. \"\"\"\n",
    "        target_sequence = self.start_token + json_tokens + self.end_token\n",
    "\n",
    "        # Remove first dimension from tensor, as it refers to the batch. Batches are automatically created by DataLoader.\n",
    "        tokenizer_response = self.processor.tokenizer(\n",
    "            target_sequence,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.model_config.decoder.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )['input_ids'].squeeze(0)\n",
    "        \n",
    "        labels = tokenizer_response.clone()\n",
    "        \n",
    "        # Replace all pad tokens by the ignore token.\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = self.ignore_id\n",
    "\n",
    "        return labels.squeeze(0), target_sequence\n",
    "\n",
    "    @staticmethod\n",
    "    def recover_from_tensor(t: torch.Tensor) -> Image:\n",
    "        \"\"\" Utility method to get back the image from a normalized tensor. Use only for debugging. \"\"\"\n",
    "        \n",
    "        arr = t.cpu().numpy()\n",
    "\n",
    "        # Donut applies mean and std transformations to image values. Reverting.\n",
    "        mean = np.array([0.5, 0.5, 0.5]).reshape(3, 1, 1)\n",
    "        std = np.array([0.5, 0.5, 0.5]).reshape(3, 1, 1)\n",
    "        arr = (arr * std + mean) * 255.0\n",
    "\n",
    "        # Limit values to uint8 range, which is the expected for images (0 to 255).\n",
    "        arr = arr.clip(0, 255).astype(np.uint8)\n",
    "\n",
    "        # [layers, width, height] -> [width, height, layers] (expected format for PIL images).\n",
    "        arr = np.transpose(arr, (1, 2, 0))\n",
    "        return Image.fromarray(arr)\n",
    "\n",
    "    def json2token(self, obj, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n",
    "        \"\"\" \n",
    "        Convert an ordered JSON object into a token sequence. \n",
    "        Source: https://github.com/clovaai/donut/blob/4cfcf972560e1a0f26eb3e294c8fc88a0d336626/donut/model.py#L499\n",
    "        \"\"\"\n",
    "        \n",
    "        if type(obj) == dict:\n",
    "            if len(obj) == 1 and \"text_sequence\" in obj:\n",
    "                return obj[\"text_sequence\"]\n",
    "            else:\n",
    "                output = \"\"\n",
    "                if sort_json_key:\n",
    "                    keys = sorted(obj.keys(), reverse=True)\n",
    "                else:\n",
    "                    keys = obj.keys()\n",
    "                for k in keys:\n",
    "                    if update_special_tokens_for_json_key:\n",
    "                        # Add to the list of special tokens detected.\n",
    "                        self.new_special_tokens.add(fr\"<s_{k}>\")\n",
    "                        self.new_special_tokens.add(fr\"</s_{k}>\")\n",
    "                    output += (\n",
    "                            fr\"<s_{k}>\"\n",
    "                            + self.json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
    "                            + fr\"</s_{k}>\"\n",
    "                    )\n",
    "                return output\n",
    "        elif type(obj) == list:\n",
    "            return r\"<sep/>\".join(\n",
    "                [self.json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
    "            )\n",
    "        else:\n",
    "            # excluded special tokens for now\n",
    "            obj = str(obj)\n",
    "            if f\"<{obj}/>\" in self.new_special_tokens:\n",
    "                obj = f\"<{obj}/>\"  # for categorical special tokens\n",
    "            return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cc6f26-66ba-44d7-adcb-13af6bc84999",
   "metadata": {},
   "source": [
    "# Creating datasets\n",
    "\n",
    "* A separate dataset is created for each split.\n",
    "* New tokens detected in all datasets will be added to Donut vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64980ce3-c0ef-4093-ab44-9f8fbb7deb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE WITH YOUR DATASET'S PATH\n",
    "DATASET_PATH = Path('synth_data')\n",
    "excluded_keys = [\n",
    "    \"drom\",\n",
    "    \"corp\",\n",
    "    \"ltype\",\n",
    "    \"ctype\",\n",
    "    \"atype\",\n",
    "    \"metgr\",\n",
    "    \"id\",\n",
    "    \"po\",\n",
    "    \"met\",\n",
    "    \"sku\",\n",
    "    \"products\"\n",
    "]\n",
    "\n",
    "training_dataset = DonutDataset(DATASET_PATH, processor, model_config, split='train', start_token=\"<s_cord-v2>\", end_token=\"</s_cord-v2>\", excluded_keys=excluded_keys)\n",
    "validation_dataset = DonutDataset(DATASET_PATH, processor, model_config, split='val', start_token=\"<s_cord-v2>\", end_token=\"</s_cord-v2>\", excluded_keys=excluded_keys)\n",
    "test_dataset = DonutDataset(DATASET_PATH, processor, model_config, split='test', start_token=\"<s_cord-v2>\", end_token=\"</s_cord-v2>\", excluded_keys=excluded_keys)\n",
    "\n",
    "# Gather all new tokens\n",
    "all_new_tokens = set()\n",
    "all_new_tokens.update(training_dataset.new_special_tokens)\n",
    "all_new_tokens.update(validation_dataset.new_special_tokens)\n",
    "all_new_tokens.update(test_dataset.new_special_tokens)\n",
    "all_new_tokens_list = list(all_new_tokens)\n",
    "\n",
    "# Add new tokens to Donut processor and model.\n",
    "newly_added_num = processor.tokenizer.add_tokens(all_new_tokens_list)\n",
    "if newly_added_num > 0:\n",
    "    model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "# Allow special tokens you use in output (e.g. task start/end), block others\n",
    "bad_words = [id for tok, id in processor.tokenizer.get_vocab().items() if tok in processor.tokenizer.all_special_tokens and tok not in [\"<s_cord-v2>\", \"</s_cord-v2>\"]]\n",
    "bad_words_ids = [[token_id] for token_id in bad_words]\n",
    "\n",
    "# Show final split percentages.\n",
    "total_samples = len(training_dataset) + len(validation_dataset) + len(test_dataset)\n",
    "print(f'Training set:   {len(training_dataset)} ({len(training_dataset) * 100// total_samples} %)')\n",
    "print(f'Validation set: {len(validation_dataset)} ({len(validation_dataset) * 100// total_samples} %)')\n",
    "print(f'Test set:       {len(test_dataset)} ({len(test_dataset) * 100// total_samples} %)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f0407f-4822-43a5-927d-aac9d7cc7797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE=4\n",
    "train_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "val_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2146522-fa36-4048-a27a-0b206c11ca10",
   "metadata": {},
   "source": [
    "# Define PyTorch Lightning module\n",
    "\n",
    "Pytorch Lightning is a PyTorch version that gives special focus to modularization, making it a bit more easy to configure than its base version.\n",
    "Here, we'll create a LightningModule to define the training behaviour of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d24170-1bea-4ce9-a2e4-445295bbc053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from nltk import edit_distance\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "\n",
    "\n",
    "class DonutModelPLModule(pl.LightningModule):\n",
    "    def __init__(self, config, processor, model, batch_size, max_length, train_dataloader_obj, val_dataloader_obj):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.train_dataloader_obj = train_dataloader_obj\n",
    "        self.val_dataloader_obj = val_dataloader_obj\n",
    "\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        outputs = self.model(pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, sync_dist=True, batch_size=self.batch_size)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, dataset_idx=0):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        answers = [a.strip().lower() for a in batch['target_str_seq']]\n",
    "\n",
    "        batch_size = pixel_values.size(0)\n",
    "\n",
    "        decoder_input_ids = torch.full(\n",
    "            (batch_size, 1),\n",
    "            self.model.config.decoder_start_token_id,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            pixel_values,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            max_length=self.max_length,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "            num_beams=1,\n",
    "            bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "        decoded = self.processor.tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "        predictions = [d.strip().lower() for d in decoded]\n",
    "\n",
    "        # Metrics\n",
    "        metrics = self._compute_metrics(predictions, answers)\n",
    "\n",
    "        print(\"Prediction vs Answer (first sample in batch):\")\n",
    "        print(\"\\n\".join([f\"\\t{k}: {v}\" for k, v in metrics.items()]))\n",
    "        print(f\"    Prediction: {predictions[0]}\")\n",
    "        print(f\"    Reference : {answers[0]}\")\n",
    "\n",
    "        self.validation_step_outputs.append(metrics)\n",
    "\n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"references\": answers,\n",
    "            **metrics\n",
    "        }\n",
    "\n",
    "    def _compute_metrics(self, predictions, answers) -> dict:\n",
    "        num_degenerated_samples = 0\n",
    "        matching_keys_ratios = list()\n",
    "        edit_distance_values = list()\n",
    "\n",
    "        for prediction, answer in zip(predictions, answers):\n",
    "            try:\n",
    "                tree_pred = self._etree_to_dict(ET.fromstring(prediction))\n",
    "                tree_ans = self._etree_to_dict(ET.fromstring(answer))\n",
    "\n",
    "                matching_keys_ratio, edit_distance = self._find_matching_keys_and_values(tree_pred, tree_ans)\n",
    "                matching_keys_ratios.append(matching_keys_ratio)\n",
    "                edit_distance_values.append(edit_distance)\n",
    "            except ET.ParseError:\n",
    "                num_degenerated_samples += 1\n",
    "\n",
    "        avg_matching_keys_ratio = sum(matching_keys_ratios) / len(matching_keys_ratios) if len(matching_keys_ratios) > 0 else 0.0\n",
    "        avg_edit_distance = sum(edit_distance_values) / len(edit_distance_values) if len(edit_distance_values) > 0 else 0.0\n",
    "        return {\n",
    "            \"degeneration_ratio\": num_degenerated_samples / len(predictions),\n",
    "            \"extraction_similarity_ratio\": avg_edit_distance,\n",
    "            \"matching_keys_ratio\": avg_matching_keys_ratio\n",
    "        }\n",
    "\n",
    "    def _etree_to_dict(self, t) -> dict:\n",
    "        \"\"\"\n",
    "        Source: https://stackoverflow.com/questions/7684333/converting-xml-to-dictionary-using-elementtree\n",
    "        \"\"\"\n",
    "        d = {t.tag: {} if t.attrib else None}\n",
    "        children = list(t)\n",
    "        if children:\n",
    "            dd = defaultdict(list)\n",
    "            for dc in map(self._etree_to_dict, children):\n",
    "                for k, v in dc.items():\n",
    "                    dd[k].append(v)\n",
    "            d = {t.tag: {k: v[0] if len(v) == 1 else v\n",
    "                for k, v in dd.items()}}\n",
    "        if t.attrib:\n",
    "            d[t.tag].update(('@' + k, v)\n",
    "                for k, v in t.attrib.items())\n",
    "        if t.text:\n",
    "            text = t.text.strip()\n",
    "            if children or t.attrib:\n",
    "                if text:\n",
    "                    d[t.tag]['#text'] = text\n",
    "            else:\n",
    "                d[t.tag] = text\n",
    "        return d\n",
    "\n",
    "    def _find_matching_keys_and_values(self, pred: dict, ans: dict):\n",
    "        if 's_cord-v2' in ans:\n",
    "            ans = ans['s_cord-v2']\n",
    "\n",
    "        matching_keys = 0\n",
    "        edit_distance_ratios = []\n",
    "        total_keys = 0\n",
    "        # Shared keys\n",
    "        for expected_key in ans:\n",
    "            if expected_key in pred:\n",
    "                matching_keys += 1\n",
    "                edit_distance_ratios.append(self._compute_similarity(pred[expected_key], ans[expected_key]))\n",
    "            total_keys += 1\n",
    "\n",
    "        # Product keys\n",
    "        if isinstance(ans.get('s_products'), list) and  isinstance(pred.get('s_products'), list):\n",
    "            for pred_product, ans_product in zip(pred['s_products'], ans['s_products']):\n",
    "                for product_key in ans_product:\n",
    "                    if product_key in pred_product:\n",
    "                        matching_keys += 1\n",
    "                        edit_distance_ratios.append(self._compute_similarity(pred_product[product_key], ans_product[product_key]))\n",
    "                    total_keys += 1\n",
    "\n",
    "        avg_edit_distance_ratio = sum(edit_distance_ratios) / len(edit_distance_ratios)\n",
    "        matching_keys_ratio = matching_keys / total_keys\n",
    "\n",
    "        return matching_keys_ratio, avg_edit_distance_ratio\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_similarity(pred_value, gt_value):\n",
    "        return 1.0 - (edit_distance(pred_value, gt_value) / max(len(pred_value), len(gt_value)))\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        all_degeneration_ratios = []\n",
    "        all_similarity_ratios = []\n",
    "        all_matching_keys_ratios = []\n",
    "\n",
    "        for track in self.validation_step_outputs:\n",
    "            all_degeneration_ratios.append(track['degeneration_ratio'])\n",
    "            all_similarity_ratios.append(track['extraction_similarity_ratio'])\n",
    "            all_matching_keys_ratios.append(track['matching_keys_ratio'])\n",
    "\n",
    "        avg_degeneration_ratio = float(np.mean(all_degeneration_ratios))\n",
    "        avg_similarity_ratio = float(np.mean(all_similarity_ratios))\n",
    "        avg_matching_keys_ratios = float(np.mean(all_matching_keys_ratios))\n",
    "\n",
    "        self.log(\"degeneration_ratio\", avg_degeneration_ratio, sync_dist=True)\n",
    "        self.log(\"similarity_ratio\", avg_similarity_ratio, sync_dist=True)\n",
    "        self.log(\"matching_keys_ratios\", avg_matching_keys_ratios, sync_dist=True)\n",
    "\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # TODO: Add a learning rate scheduler\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.get(\"lr\"))\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_dataloader_obj\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_dataloader_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a883157a-4214-4feb-801e-e2e30029f1eb",
   "metadata": {},
   "source": [
    "# Set training configuration parameters\n",
    "\n",
    "## Parameters list:\n",
    "\n",
    "* `output_dir`: local folder in where the training output should be saved.\n",
    "* `num_train_epochs`: An epoch refers to one complete pass through the entire training dataset. Multiple epochs allow the model to improve generalization, adjusting better to the seen data. Too much epochs can lead to overfitting. More info [here](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/#:~:text=at%20an%20epoch.-,What%20Is%20an%20Epoch%3F,-The%20number%20of)\n",
    "* `learning_rate`: controls how fast a model can adjust its parameters. Low values can cause a slow training, while big values can cause unstable results. More info [here](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/#:~:text=The-,learning%20rate,-controls%20how%20quickly)\n",
    "* `per_device_train_batch_size`: Batch size. A batch is the number of samples that the model sees before updating its weights. It helps to reduce variance, but high values imply fore VRAM usage. Especially, when dealing with big images, this can produce memory explosion. More info [here](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/#:~:text=batch%20gradient%20descent.-,How%20to%20Configure%20Mini%2DBatch%20Gradient%20Descent,-Mini%2Dbatch%20gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b49e0-730b-4d77-9d2e-d5f2069397e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"max_epochs\": 80,\n",
    "    \"val_check_interval\": 1.0,\n",
    "    \"check_val_every_n_epoch\": 1,\n",
    "    \"gradient_clip_val\": 1.0,\n",
    "    \"lr\": 2e-6,\n",
    "    \"num_nodes\": 1,\n",
    "    \"precision\": \"16-mixed\",\n",
    "    \"log_every_n_steps\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dcc0a8-98ac-4486-b1ef-0c05199b9386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "\n",
    "# login()  # Use itam-franmr account token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aabdcd-3000-4fe3-bf64-20b3658e3845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import Callback, EarlyStopping\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"Donut\", name=\"demo_run_donut_synth_v2\")\n",
    "\n",
    "\n",
    "class PushToHubCallback(Callback):\n",
    "    def __init__(self, logger, hf_repo, ckpt_dirpath):\n",
    "        super().__init__()\n",
    "        self.logger = logger\n",
    "        self.hf_repo = hf_repo\n",
    "        self.ckpt_dirpath = ckpt_dirpath\n",
    "        self.api = HfApi()\n",
    "\n",
    "    def _push_checkpoint_file(self):\n",
    "        if os.path.exists(self.ckpt_dirpath):\n",
    "            ckpt_name = self._get_last_checkpoint(self.ckpt_dirpath)\n",
    "            ckpt_path = os.path.join(self.ckpt_dirpath, ckpt_name)\n",
    "            print(\"Checkpoint selected:\", ckpt_path)\n",
    "            self._remove_old_checkpoints_from_hub()\n",
    "            self.api.upload_file(\n",
    "                path_or_fileobj=ckpt_path,\n",
    "                path_in_repo=os.path.join('checkpoints', ckpt_name),\n",
    "                repo_id=self.hf_repo,\n",
    "                repo_type='model',\n",
    "                commit_message=f\"Checkpoint saved: {ckpt_name}\"\n",
    "            )\n",
    "        else:\n",
    "            self.logger.warning(\"No checkpoints available.\")\n",
    "\n",
    "    def _remove_old_checkpoints_from_hub(self):\n",
    "        all_files = self.api.list_repo_files(repo_id=self.hf_repo, repo_type='model')\n",
    "        ckpt_files = [f for f in all_files if 'checkpoints/epoch' in f]\n",
    "\n",
    "        for ckpt_file in ckpt_files:\n",
    "            print(\"Removing old checkpoint:\", ckpt_file)\n",
    "\n",
    "            self.api.delete_file(\n",
    "                path_in_repo=ckpt_file,\n",
    "                repo_id=self.hf_repo,\n",
    "                repo_type='model',\n",
    "                commit_message=f'Removed old checkpoint \"{ckpt_file}\"',\n",
    "            )\n",
    "\n",
    "    def _get_last_checkpoint(self, ckpt_dir: str):\n",
    "        ckpts = os.listdir(ckpt_dir)\n",
    "        print(\"Checkpoints saved:\", ckpts)\n",
    "        if not len(ckpts):\n",
    "            return None\n",
    "        elif len(ckpts) == 1:\n",
    "            return ckpts[0]\n",
    "        else:\n",
    "            epoch_step_pairs = []\n",
    "            for ckpt in ckpts:\n",
    "                matches = re.match(r\"epoch=(\\d*)_step=(\\d*).ckpt\", ckpt)\n",
    "                if matches:\n",
    "                    epoch, step = map(lambda x: int(x), matches.groups())\n",
    "                    epoch_step_pairs.append([epoch, step])\n",
    "            best_ckpt_values = self._find_highest_list(epoch_step_pairs)\n",
    "            best_ckpt_idx = epoch_step_pairs.index(best_ckpt_values)\n",
    "            return ckpts[best_ckpt_idx]\n",
    "            \n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        self.logger.info(f\"Pushing model to the Hub [epoch {trainer.current_epoch}]\")\n",
    "        self._push_checkpoint_file()\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        self.logger.info(f\"Pushing model to the hub after training.\")\n",
    "        pl_module.processor.push_to_hub(self.hf_repo, commit_message=\"Training Finished!\")\n",
    "        pl_module.model.push_to_hub(self.hf_repo, commit_message=\"Training Finished!\")\n",
    "\n",
    "    def _find_highest_list(self, lists: list, curr_idx=0):\n",
    "        if len(lists) == 0:\n",
    "            return []\n",
    "\n",
    "        lists_lengths = [len(i) for i in lists]\n",
    "\n",
    "        if not all(x == lists_lengths[0] for x in lists_lengths):\n",
    "            raise ValueError(\"All lists must have the same length\")\n",
    "\n",
    "        length = lists_lengths[0]\n",
    "\n",
    "        lists_idxs_with_max_value = []\n",
    "        curr_max = -1\n",
    "\n",
    "        for i, l in enumerate(lists):\n",
    "            if l[curr_idx] > curr_max:\n",
    "                curr_max = l[curr_idx]\n",
    "                lists_idxs_with_max_value.clear()\n",
    "                lists_idxs_with_max_value.append(i)\n",
    "            elif l[curr_idx] == curr_max:\n",
    "                lists_idxs_with_max_value.append(i)\n",
    "\n",
    "        if len(lists_idxs_with_max_value) == 1:\n",
    "            return lists[lists_idxs_with_max_value[0]]\n",
    "        else:\n",
    "            curr_idx += 1\n",
    "            if curr_idx > length - 1:\n",
    "                return lists[lists_idxs_with_max_value[0]]\n",
    "\n",
    "            return self._find_highest_list([lists[j] for j in lists_idxs_with_max_value], curr_idx)\n",
    "\n",
    "\n",
    "model_checkpoint_dirpath = 'checkpoints'\n",
    "self.push_to_hub_callback = PushToHubCallback(\n",
    "    self.logger,\n",
    "    self.hf_repo,\n",
    "    model_checkpoint_dirpath\n",
    ")\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=model_checkpoint_dirpath,\n",
    "    filename='{epoch}_{step}',\n",
    "    save_top_k=1,\n",
    "    every_n_epochs=1,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "# early_stop_callback = EarlyStopping(monitor=\"val_edit_distance\", patience=3, verbose=False, mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=config.get(\"max_epochs\"),\n",
    "    val_check_interval=config.get(\"val_check_interval\"),\n",
    "    check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
    "    gradient_clip_val = config.get(\"gradient_clip_val\"),\n",
    "    precision=config.get(\"precision\"),\n",
    "    num_sanity_val_steps=0,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[PushToHubCallback()],\n",
    "    log_every_n_steps=config.get('log_every_n_steps')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24207081-1604-4b08-b2cb-3cfa6fbc0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_module = DonutModelPLModule(config, processor, model)\n",
    "trainer.fit(model_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa4d3a-2365-4734-93d1-157507d83139",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc64a8dc-2c3e-4ff5-b46d-3b972016a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "# processor = DonutProcessor.from_pretrained(\"itam-franmr/donut_clem\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"itam-franmr/donut_clem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c86f4a-c145-4d2d-9c40-f26188c47205",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q donut-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29096f0b-f98d-4d10-9c9f-d8d2463c87fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from donut import JSONParseEvaluator\n",
    "\n",
    "model.eval()  # Activates the evaluation mode\n",
    "\n",
    "output_list = []\n",
    "accs = []\n",
    "\n",
    "for idx, sample in tqdm(enumerate(validation_dataset), total=len(validation_dataset)):\n",
    "    # Prepare encoder inputs\n",
    "    pixel_values = sample[\"pixel_values\"]\n",
    "    pixel_values = pixel_values.unsqueeze(0)\n",
    "\n",
    "    # Prepare decoder inputs\n",
    "    task_prompt = \"<s_cord-v2>\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "    decoder_input_ids = decoder_input_ids\n",
    "\n",
    "    # Autoregressively generate sequences\n",
    "    outputs = model.generate(\n",
    "        pixel_values,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        num_beams=1,\n",
    "        # bad_words=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    # Turn response into JSON format\n",
    "    seq = processor.batch_decode(outputs.sequences)[0]\n",
    "    seq = seq.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()\n",
    "    seq = processor.token2json(seq)\n",
    "\n",
    "    ground_truth = sample[\"target_str_seq\"]\n",
    "    evaluator = JSONParseEvaluator()\n",
    "    score = evaluator.cal_acc(seq, ground_truth)\n",
    "\n",
    "    accs.append(score)\n",
    "    output_list.append(seq)\n",
    "\n",
    "scores = {\"accuracies\": accs, \"mean_accuracy\": np.mean(accs)}\n",
    "print(scores, f\"length: {len(accs)}\")\n",
    "print(f\"Mean accuracy: {scores['mean_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ee4cf-68cc-4bc8-a9f2-cfc280d58989",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516357d7-7cf9-4193-b063-71b665a827ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import transformers\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Load processor and model\n",
    "processor = DonutProcessor.from_pretrained(model_repository_name)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_repository_name)\n",
    "\n",
    "# Load random document image from test set\n",
    "test_sample = test_dataset[random.randint(0, len(test_dataset))]\n",
    "\n",
    "pixel_values = sample['pixel_values'].unsqueeze(0)\n",
    "task_prompt = \"<s_cord-v2>\"\n",
    "decoder_input_ids = sample['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c673a0-f64b-4cc9-826e-44ed12e10336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Load random document image from test set\n",
    "test_sample = test_dataset[random.randint(0, len(test_dataset))]\n",
    "\n",
    "pixel_values = test_sample['pixel_values'].unsqueeze(0)\n",
    "task_prompt = \"<s_cord-v2>\"\n",
    "decoder_input_ids = test_sample['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d06b2-ca50-404b-bae5-2ad22e87832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    pixel_values,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    max_length=model.decoder.config.max_position_embeddings,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    "    num_beams=1,\n",
    "    bad_words_ids=bad_words_ids,\n",
    "    return_dict_in_generate=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f2f240-d339-4b74-bf99-91e9ec1629d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process output\n",
    "prediction = processor.batch_decode(outputs.sequences)[0]\n",
    "prediction = processor.token2json(prediction)\n",
    "\n",
    "# Load reference target\n",
    "target = processor.token2json(test_sample['output_target_seq'])\n",
    "\n",
    "print(f\"Reference:\\n {target}\")\n",
    "print(f\"Prediction:\\n {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
